Cette partie pr√©sente une m√©thodologie innovante visant √† reconstruire l‚Äôexhaustivit√© de la base d‚Äôapprentissage en tirant parti des LLM, et plus particuli√®rement des mod√®les comp√©titifs publi√©s en open source. L‚Äôapproche s‚Äôappuie sur le mode op√©ratoire des annotateurs, les notes explicatives de la DNE et l‚Äôinspiration de projets r√©ussis dans d‚Äôautres INS, comme @ClassifAI. Les technologies cloud et la puissance de calcul du datalab SSP Cloud permettent de tester cette approche prometteuse et de l‚Äôadapter aux besoins m√©tier sp√©cifiques de la base Sirene, en int√©grant le passage √† la NAF 2025.

L‚Äôutilisation de donn√©es non-sensibles √©vite les contraintes d‚Äôanonymisation, et l‚Äôauto-h√©bergement avec les technologies cloud[^ref-techno-cloud] assure ind√©pendance et reproductibilit√© sur des infrastructures similaires pour tous les utilisateurs d'Onyxia [^k8s-cluster].

[^k8s-cluster]: ou plus largement de cluster Kubernetes

## M√©thodologie 

L‚Äôobjectif n‚Äôest pas de d√©velopper un classifieur bas√© sur un LLM, mais de produire une base annot√©e en NAF 2025 suffisamment large et qualitative pour entra√Æner ult√©rieurement un mod√®le de classification. Con√ßue comme une op√©ration ponctuelle et sans contrainte de mise en production, l‚Äôexp√©rimentation a b√©n√©fici√© d‚Äôune grande libert√© technique, permettant d‚Äôinnover, d‚Äôit√©rer rapidement et de se concentrer sur les approches les plus pertinentes pour construire la base d‚Äôapprentissage NAF 2025.

Le principe consiste √† utiliser les LLM comme des _annotateurs virtuels_, en reproduisant la d√©marche d√©crite pr√©c√©demment. √Ä partir des notes explicatives, le LLM attribue le code NAF 2025 le plus appropri√© parmi les choix propos√©s et, lorsqu‚Äôil ne peut statuer, classe le cas comme incodable. La base recod√©e correspond donc √† l‚Äôensemble des unit√©s que le LLM parvient √† annoter : une partie restera incodable et exclue, mais une proportion substantielle de la base multivoque est recod√©e, constituant ainsi une premi√®re version de la base d‚Äôapprentissage NAF 2025, destin√©e √† l‚Äôentra√Ænement d‚Äôun futur classifieur.

√Ä partir de la base d‚Äôapprentissage initiale en NAF 2008, la d√©marche consiste d‚Äôabord √† s√©parer les unit√©s univoques des unit√©s multivoques √† l‚Äôaide de la table de correspondance. Les univoques sont ensuite directement recod√©es et constituent une premi√®re composante de la future base d‚Äôapprentissage. Les multivoques font, quant √† elles, l‚Äôobjet d‚Äôun traitement sp√©cifique: apr√®s suppression des libell√©s dupliqu√©s afin de r√©duire le nombre d‚Äôinf√©rences, un LLM est mobilis√© comme _annotateur virtuel_. Comme le mod√®le ne dispose pas de l‚Äôexpertise m√©tier des annotateurs, et encore moins en NAF 2025 qui est encore provisoire, son contexte est enrichi en lui fournissant, pour chaque observation, la liste des codes NAF 2025 envisageables accompagn√©s de leurs notes explicatives. Plusieurs approches d‚Äôaugmentation du contexte seront pr√©sent√©es, notamment le RAG[^remark-rag] et le CAG ; ce dernier sera privil√©gi√©, car il r√©pond le mieux aux besoins m√©tier tout en minimisant le co√ªt d‚Äôinf√©rence. Une phase d‚Äô√©valuation permet ensuite d‚Äôidentifier le LLM dont les annotations sont les plus align√©es avec la v√©rit√© terrain issue de la campagne d‚Äôannotation manuelle. Enfin, les multivoques ainsi recod√©es sont r√©unies avec les univoques, afin de constituer la nouvelle base d‚Äôapprentissage en NAF 2025 destin√©e √† entra√Æner le futur classifieur.

[^remark-rag]: Le RAG (Retrieval‚ÄëAugmented Generation) d√©crit plus se montre tr√®s prometteur. Cependant, son int√©gration reste aujourd‚Äôhui confront√©e √† des d√©fis techniques et m√©tier (temps d'inf√©rence, √©valuation et efficacit√© du retrieval, performance globale avec la g√©n√©ration). Pour ces raisons, le RAG est encore en phase de recherche et de prototypage dans notre contexte. Les r√©sultats associ√©s ne seront pas pr√©sent√©s dans cet article.
[^ref-techno-cloud]: Pour en savoir plus sur l‚Äôapport des technologies cloud √† la statistique publique, il est recommand√© de consulter l‚Äôarticle de l‚Äô√©dition JMS de cette ann√©e @cloudprod.

::: {.center}
![Approche globale pour construire le mod√®le en NAF 2025](img/methodo-nace-rev.png){#fig-methodo-nace-rev}

*Le sch√©ma illustre fastText comme mod√®le de classification, car il a servi aux premi√®res it√©rations, mais la m√©thodologie est ind√©pendante du mod√®le : une fois la base d‚Äôapprentissage NAF 2025 constitu√©e, tout autre classifieur supervis√© peut √™tre utilis√©, notamment ceux de la librairie torchTextClassifiers ou d‚Äôarchitectures plus r√©centes adapt√©es au contexte m√©tier.*
:::

## Le datalab SSP‚ÄØCloud comme plateforme d‚Äôexp√©rimentation

Le datalab **SSP‚ÄØCloud** n‚Äôest pas destin√© √† h√©berger un LLM en production‚ÄØ; il s‚Äôagit d‚Äôun environnement **exp√©rimental et ponctuel** qui permet de tester des mod√®les et des pipelines sans les contraintes d‚Äôune mise en service continue. Cette orientation exp√©rimentale s‚Äôinscrit naturellement dans une d√©marche **open‚Äësource**, car elle favorise la transparence, la r√©utilisabilit√© du code et la ma√Ætrise des co√ªts.

D√©velopp√©e √† l‚ÄôInsee, la plateforme SSP‚ÄØCloud repose sur une **infrastructure normalis√©e**‚ÄØ: tous les serveurs partagent la m√™me configuration mat√©rielle et utilisent la m√™me distribution Linux (Debian). Cette homog√©n√©it√© garantit la **reproductibilit√©** des environnements de travail et facilite le partage des r√©sultats entre √©quipes. En outre, le SSP‚ÄØCloud s‚Äôappuie sur un **cluster Kubernetes** qui constitue le socle d‚Äôune infrastructure robuste. Gr√¢ce √† ce cluster, le d√©ploiement automatis√© d‚Äôapplications potentiellement intensives en donn√©es devient possible, ce qui permet de **simuler un v√©ritable environnement de production** tout en conservant la flexibilit√© d‚Äôun cadre de test.

Par ailleurs, le SSP‚ÄØCloud a √©t√© construit selon les **standards les plus r√©cents des infrastructures de data‚Äëscience**. Concr√®tement, les services sont lanc√©s sous forme de **conteneurs Docker**‚ÄØ; ainsi, chaque composant est empaquet√© avec ses d√©pendances, ce qui assure une forte **reproductibilit√© des d√©ploiements**. Cette approche implique toutefois une phase de d√©veloppement l√©g√®rement plus co√ªteuse, car elle requiert la d√©finition et la gestion des images Docker. Le datalab adopte √©galement une philosophie **cloud‚Äënative**‚ÄØ: il est organis√© autour d‚Äôun ensemble **modulaire de briques logicielles** (code, donn√©es, configuration, environnement d‚Äôex√©cution). Cette s√©paration nette constitue un principe majeur des bonnes pratiques, car elle facilite la maintenance, le scaling et l‚Äôint√©gration de nouveaux outils.

Sur le plan de la **souverainet√© et de l‚Äôind√©pendance**, toutes les technologies utilis√©es sont **auto‚Äëh√©berg√©es et open‚Äësource**. M√™me si les jeux de donn√©es manipul√©s ne sont pas sensibles, aucune offre propri√©taire n‚Äôest employ√©e, ce qui pr√©serve la ma√Ætrise totale du syst√®me. Il convient de pr√©ciser que, conform√©ment √† la politique de la plateforme, **seules des donn√©es non sensibles**[^remarque-onyxia-interne] (donn√©es publiques, jeux de donn√©es synth√©tiques, donn√©es agr√©g√©es anonymis√©es, etc.) peuvent √™tre d√©pos√©es ou trait√©es sur le datalab‚ÄØ; les donn√©es √† caract√®re personnel ou confidentielles sont strictement exclues. En adoptant une **approche frugale**, nous s√©lectionnons des mod√®les open‚Äësource adapt√©s √† nos besoins pour une p√©riode d√©termin√©e, r√©duisant ainsi les d√©penses li√©es aux licences de solutions commerciales. L‚Äôauto‚Äëh√©bergement nous garantit en outre un **contr√¥le complet sur nos donn√©es** et un acc√®s √† des mod√®les affichant des performances comparables √† celles des solutions propri√©taires, sp√©cifiquement pour notre cas d‚Äôusage.

Enfin, le SSP‚ÄØCloud met √† disposition des **ressources mat√©rielles pr√©cieuses**. Chaque projet b√©n√©ficie d‚Äôun espace de calcul pouvant atteindre **30‚ÄØCPU**, ainsi que d‚Äôun acc√®s sur demande √† des **GPU NVIDIA H100** (PCI‚Äëe, NVLink), ressources rares mais essentielles pour les traitements IA massifs. Le stockage est assur√© par un **espace S3**, offrant une capacit√© √©lev√©e et une grande disponibilit√© pour le versionnage et le partage des jeux de donn√©es.

En r√©sum√©, le datalab SSP‚ÄØCloud combine normalisation, modularit√©, souverainet√© et puissance de calcul, tout en imposant la restriction √† des donn√©es non sensibles, ce qui en fait une plateforme id√©ale pour mener des exp√©rimentations IA rigoureuses, ma√Ætriser les co√ªts et respecter les exigences de la fonction publique.

[^remarque-onyxia-interne]: Pour le traitement de donn√©es strat√©giques, il est possible d‚Äôinstancier Onyxia en interne pour avoir son propre datalab et manipuler des donn√©es sensibles √† l'instar de _Nubo_, le cloud priv√© s√©curis√© de la DGFIP.

### Des services √† disposition pour les traitements IA

Le SSP Cloud propose un catalogue de services vari√©s, adapt√©s aux besoins des traitements IA. Certaines solutions sont particuli√®rement pertinentes pour les workflows de LLM et pour l‚Äôinf√©rence sur de larges volumes de donn√©es. L‚Äôobjectif est de pr√©senter bri√®vement les concepts cl√©s, les exemples disponibles dans le datalab et ceux qui seront exploit√©s dans ce cadre.

#### Bases de donn√©es vectorielles

Les bases de donn√©es vectorielles permettent de stocker et d‚Äôinterroger des repr√©sentations vectorielles de donn√©es, indispensables pour la recherche s√©mantique et l‚Äôanalyse par similarit√©. Sur le datalab, plusieurs solutions sont propos√©es, pour en citer quelques-unes, Milvus, ChromaDB et Qdrant, cette derni√®re √©tant retenue pour les travaux pr√©sent√©s. Ce type de base peut notamment servir √† conserver les repr√©sentations vectorielles de notes explicatives g√©n√©r√©es par un mod√®le de type transformer ou par un LLM sp√©cialis√© dans l‚Äôembedding, afin de faciliter des requ√™tes s√©mantiques pertinentes sur de larges corpus.

Au-del√† du simple stockage, l‚Äôenjeu r√©side dans la capacit√© √† interroger efficacement ces vecteurs. En effet, un dataframe se limiterait √† conserver les embeddings et imposerait des comparaisons successives entre vecteurs, ce qui devient rapidement inefficace lorsque le volume de donn√©es augmente. √Ä l‚Äôinverse, une base de donn√©e vectorielle comme _Qdrant_ int√®gre des structures d‚Äôindexation d√©di√©es permettant de retrouver rapidement les vecteurs les plus proches sur le plan s√©mantique. Cette capacit√© est d√©terminante pour interroger un nombre important d‚Äôembeddings de mani√®re performante, par exemple pour identifier les notes explicatives les plus similaires au sein d‚Äôun corpus volumineux.

#### Serveurs d‚Äôinf√©rence optimis√©s

Les serveurs d‚Äôinf√©rence permettent l‚Äôex√©cution performante des mod√®les LLM. Plusieurs projets open source existent, dont _Llama.cpp_ et _Ollama_, mais _vLLM_ se distingue par son optimisation sur GPU et sa capacit√© √† g√©rer l‚Äôinf√©rence sur des millions de donn√©es. Cette solution est particuli√®rement adapt√©e au traitement par batch, un domaine r√©cent pr√©sentant des d√©fis sp√©cifiques, notamment en termes de vitesse et de scalabilit√©. 

Les moteurs d‚Äôinf√©rence classiques fonctionnent avec un **batching statique**, comparable √† une cha√Æne de production traditionnelle : un lot de requ√™tes est trait√©, puis l‚Äôop√©ration s‚Äôarr√™te avant de passer au lot suivant. Cette approche entra√Æne des latences, notamment lorsque de nouvelles requ√™tes arrivent alors qu‚Äôun batch est d√©j√† en cours de traitement.

√Ä l‚Äôinverse, _vLLM_ adopte le principe du **continuous batching** : au lieu d‚Äôattendre que toutes les s√©quences d‚Äôun lot soient termin√©es, les requ√™tes finalis√©es sont imm√©diatement remplac√©es par de nouvelles, √† chaque it√©ration. Cela permet de maintenir les GPU constamment occup√©s, d‚Äôaugmenter drastiquement le d√©bit de traitement, de r√©duire la latence et d‚Äôam√©liorer l‚Äôutilisation des ressources. L‚Äôanalogie serait celle d‚Äôun serveur de restaurant prenant les commandes de plusieurs tables en continu, plut√¥t que de multiplier les allers-retours pour chaque commande individuellement. Gr√¢ce √† cette approche dynamique, vLLM est particuli√®rement adapt√© aux traitements portant sur des millions de donn√©es, en particulier lorsqu‚Äôune ex√©cution en batch est requise.

#### Gestionnaire de prompts

Les gestionnaires de prompts permettent de versionner, tracer et √©valuer les prompts afin d‚Äôoptimiser l‚Äôinteraction avec les LLM. Dans le datalab, la plateforme _Langfuse_, open source, est utilis√©e pour ces t√¢ches. Dans le cadre de ce projet, il sera crucial de trouver les bonnes formulations de consignes pour les LLM afin d‚Äôassurer que le comportement attendu soit respect√©, tout en conservant la tra√ßabilit√© et la reproductibilit√© des exp√©rimentations.

#### Gestionnaire de mod√®les

Les artefacts des mod√®les sont centralis√©s dans un model store, ici g√©r√© via _MLflow_, permettant de suivre les versions, les d√©ploiements et la tra√ßabilit√© des mod√®les utilis√©s dans les workflows. Cette gestion garantit un suivi pr√©cis des mod√®les employ√©s pour les diff√©rents traitements IA et facilite la reproductibilit√© des r√©sultats.

### Les enjeux de l'inf√©rence LLM par batch

L‚Äôinf√©rence d√©signe le processus par lequel un mod√®le de langage (LLM) g√©n√®re des pr√©dictions ou des r√©ponses √† partir d‚Äôentr√©es donn√©es. Dans une inf√©rence unitaire classique, chaque requ√™te est trait√©e individuellement. M√™me dans ce cadre, plusieurs facteurs influencent d√©j√† la performance et la fiabilit√©, tels que le mod√®le utilis√©, la formulation du prompt, le serveur d‚Äôinf√©rence ou les sp√©cificit√©s mat√©rielles. Ces √©l√©ments d√©terminent le temps de r√©ponse et peuvent provoquer des ralentissements ou des erreurs, mais restent g√©n√©ralement ma√Ætrisables lorsque les volumes de donn√©es sont limit√©s.

En revanche, lorsque l‚Äôinf√©rence est effectu√©e par batch, des contraintes suppl√©mentaires apparaissent. Le temps d‚Äôinf√©rence n‚Äôest jamais lin√©aire et le traitement simultan√© de plusieurs √©l√©ments peut g√©n√©rer des variations importantes de performance. La vitesse globale est fortement impact√©e, le risque d‚Äôinterruption augmente en cas d‚Äôerreur sur un √©l√©ment, et la capacit√© de traitement devient limit√©e par l‚Äôinfrastructure locale ou par d‚Äô√©ventuelles restrictions des services externes. Par ailleurs, le traitement de larges volumes de donn√©es entra√Æne des charges mat√©rielles et financi√®res significatives.

Afin de r√©pondre √† ces enjeux, une approche enti√®rement hors-ligne est adopt√©e, avec l‚Äôimportation locale de toutes les technologies n√©cessaires. Cette m√©thode permet d‚Äô√©liminer les ralentissements et interruptions li√©s aux flux r√©seau, de traiter de tr√®s grands volumes de donn√©es sans √™tre limit√© par des quotas ou des restrictions d‚ÄôAPI, et d‚Äôoptimiser la parall√©lisation des t√¢ches pour r√©duire le temps de traitement. Elle permet √©galement de ma√Ætriser pleinement les co√ªts et l‚Äôinfrastructure.

Ainsi, l‚Äôinf√©rence par batch devient plus rapide, fiable et scalable. √Ä titre d‚Äôexemple, l‚Äôapplication de cette m√©thodologie sur une base de 2 millions de donn√©es n√©cessite jusqu‚Äô√† 5 jours, bien que des optimisations aient permis de r√©duire ce temps de mani√®re significative, ce qui souligne l‚Äôimportance strat√©gique d‚Äôune telle organisation.

## D√©tail des √©tapes et concepts cl√©s appliqu√©s

### √âtape 1: Pr√©parer les donn√©es √† disposition

La premi√®re √©tape consiste √† rassembler et structurer l‚Äôensemble des donn√©es n√©cessaires √† l‚Äôannotation automatique par LLM. Les sources principales incluent les extractions pr√©par√©es par le m√©tier, issues du Syst√®me Sirene version 4, et destin√©es √† √™tre recod√©es en NAF 2025, constituant ainsi la base d‚Äôapprentissage initiale.

S‚Äôy ajoutent les ressources des experts de la nomenclature de la DNE, en particulier la table de correspondance th√©orique et les notes explicatives, qui apportent un contexte m√©tier crucial pour traiter les cas multivoques ou ambigus. L‚Äôannotation manuelle effectu√©e lors de la campagne nationale fournit une premi√®re v√©rit√© terrain (ground truth), servant de r√©f√©rence pour √©valuer la qualit√© des pr√©dictions g√©n√©r√©es par les mod√®les.

Selon la strat√©gie choisie pour enrichir le contexte, les donn√©es sont organis√©es diff√©remment. Avec la strat√©gie RAG (Retrieval-Augmented Generation), elles sont stock√©es dans une base de donn√©es vectorielle, permettant une recherche efficace de passages pertinents pour chaque libell√©. Avec la strat√©gie CAG (Context-Augmented Generation), elles sont encapsul√©es sous forme d‚Äôobjets en programmation orient√©e objet, ce qui facilite la manipulation et l‚Äôexploitation des informations lors de l‚Äôinf√©rence.

Les notions de RAG et CAG seront d√©finies plus en d√©tail ult√©rieurement, afin de pr√©ciser leur impact sur l‚Äôannotation automatique.

### √âtape 2 : Chargement de LLM

Un **LLM (Large Language Model)** repose sur l‚Äôarchitecture transformer, introduite dans l‚Äôarticle _Attention Is All You Need_ @attentionisallyouneed. Cette architecture est centr√©e sur le m√©canisme d‚Äôauto-attention, qui permet au mod√®le de pond√©rer l‚Äôimportance relative de chaque √©l√©ment d‚Äôune s√©quence lorsqu‚Äôil produit une sortie. Dans les LLM, cette structure est largement √©tendue : de tr√®s nombreux param√®tres sont entra√Æn√©s pour pr√©dire la probabilit√© d‚Äôapparition d‚Äôun mot ou d‚Äôun token √† partir de son contexte. Plusieurs mod√®les de type GPT existent, diff√©renci√©s par leur taille, le volume de donn√©es d‚Äôentra√Ænement et la finesse de leurs repr√©sentations internes.

Le mod√®le pr√©-entra√Æn√© dispose de repr√©sentations internes complexes qui codent de fa√ßon compress√©e des connaissances extraites du texte utilis√© lors de son entra√Ænement. Il ne s‚Äôagit pas d‚Äôune base de connaissances explicite ou consultable comme une base de donn√©es classique, mais d‚Äôune structure de param√®tres qui capture des r√©gularit√©s statistiques et s√©mantiques. Cette compression de l‚Äôinformation permet au LLM de g√©n√©rer des r√©ponses coh√©rentes, de compl√©ter du texte ou d‚Äôattribuer des classes aux donn√©es, en mobilisant ces repr√©sentations implicites.

Cette perspective permet de comprendre pourquoi la taille du mod√®le et la richesse de ses donn√©es d‚Äôentra√Ænement jouent un r√¥le central dans ses performances. La course actuelle √† la puissance de calcul et √† la multiplication des param√®tres suit cette logique, comme l‚Äôillustre l'essai _The Bitter Lesson_ @thebitterlesson.

Pour ce qui concerne les LLM √† utiliser, la recherche se fait sur _Hugging Face_, la plateforme la plus r√©pandue √† l‚Äôheure actuelle pour acc√©der aux mod√®les de langage open source. Elle propose un large catalogue de mod√®les pr√©-entra√Æn√©s et fournit des outils pratiques pour leur t√©l√©chargement et leur int√©gration dans diff√©rents environnements exp√©rimentaux.

### √âtape 3: Param√©trer le LLM

#### Prompting

Qu'est ce qu'un prompt ? description textuelle de la t√¢che qu'une IA doit effectuer

##### Donner une feuille de route au LLM: le prompt syst√®me, un prompt structurant

un prompt particulier
Prompt syst√®me:

_manuel d‚Äôinstruction lu par les IA g√©n√©ratives avant de vous r√©pondre_

```
Tu es un expert de la Nomenclature statistique des Activit√©s √©conomiques dans la Communaut√© Europ√©enne (NACE). Tu es charg√© de r√©aliser le changement de nomenclature. Ta mission consiste √† attribuer un code NACE 2025 √† une entreprise, en t'appuyant sur le descriptif de son activit√© et √† partir d'une liste de codes propos√©s (identifi√©e √† partir de son code NACE 2008 existant). Voici les instructions √† suivre :
1. Analyse la description de l'activit√© principale de l'entreprise et le code NACE 2008 fourni par l'utilisateur.
2. √Ä partir de la liste des codes NACE 2025 disponible, identifie la cat√©gorie la plus appropri√©e qui correspond √† l'activit√© principale de l'entreprise.
3. Retourne le code NACE 2025 au format JSON comme sp√©cifi√© par l'utilisateur. Si la description de l'activit√© de l'entreprise n'est pas suffisamment pr√©cise pour identifier un code NACE 2025 ad√©quat, retourne `null` dans le JSON.
4. √âvalue la coh√©rence entre le code NACE 2008 fourni et la description de l'activit√© de l'entreprise. Si le code NACE 2008 ne semble pas correspondre √† cette description, retourne `False` dans le champ `nace08_valid` du JSON. Note que si tu arrives √† classer la description de l'activit√© de l'entreprise dans un code NACE 2025, le champ `nace08_valid` devrait `True`, sinon il y a incoh√©rence.
5. R√©ponds seulement avec le JSON compl√©t√© aucune autres information ne doit √™tre retourn√©.
```
##### Le prompt utilisateur, une requ√™te contextuelle au LLM
 Prompt utilisateur ou Requ√™te:
 - Un prompt [**sp√©cifique**]{.orange} pour chaque observations comprenant : 
  - le [**libell√©**]{.blue2} de l'activit√© principale de l'entreprise
  - l'ancien [**code NAF 2008**]{.blue2} connu
  - La [**liste des codes possibles**]{.blue2} issues du mapping avec leurs [**notes explicatives**]{.blue2}

- Une [**instruction sur le format**]{.orange} de r√©ponse attendu

### √âtape 4: Apporter le contexte au LLM

G√©n√©ration augment√©e, insufissance de la base de connaissance

#### RAG

@rag_paper

#### CAG 

@cag_paper

Quelle stat√©gie utiliser ?

R√©ponse: contexte m√©tier 

Lequel utiliser ? Don't do RAG ? Oui et non, √† nuancer mais du coup

avantage/inconv√©nients: retrieval imm√©diat mais propagation de l'erreur en NAF rev 2
Int√©ressant si bonne classification en NAF rev 2, notre cas donc CAG

### √âtape 5: G√©n√©rer les annotations

Contr√¥ler la sortie du LLM: sp√©cification et formatage

Qualifier la r√©ponse g√©n√©r√©e

- Tendance des LLMs a √™tre tr√®s [**volubiles**]{.orange}...
- Construction d'une [**r√©ponse type**]{.orange}, claire et br√®ves dans un [**format sp√©cifique**]{.orange}
- [**Format JSON**]{.orange} est le plus abondamment utilis√© dans l'√©cosyst√®me

```json
{
    "codable": true,
    "nace_2008_valid": true,
    "nace2025": "0147J" 
}
```


- [**Parsing**]{.orange} de la r√©ponse :
  1. [**V√©rification**]{.blue2} du format
  2. Contr√¥le des [**hallucinations**]{.blue2}

### √âtape 6 : √âvaluation des LLM

#### Quels LLM utiliser ?

| Mod√®le | Taille | Vitesse d'inf√©rence | Performance | Caract√©ristique |
| --------- | ----------- | ---------- | ------------- | ------- |
| Qwen 2.5 | 32B | [Lent]{.orange} | [Bonnes performances]{.green2} | Tr√®s restrictif |
| Ministral | 8B | [Extr√™mement rapide]{.green2} | [Tr√®s raisonnable]{.orange} | Pas restrictif |
| Mistal Small | 44B | [Lent]{.orange} | [Bonnes performances]{.green2} | Assez restrictif |
| Llama 3.1 | 70B (quantis√©) | [Extr√™mement lent]{.red2} | [Tr√®s bonnes performances]{.green2} | Assez restrictif |

La s√©lection des Large Language Models (LLM) s'est concentr√©e sur les mod√®les les plus reconnus, consid√©r√©s comme State-of-the-Art (SOTA) √† un moment donn√©, ou **disponibles en open source**. Il est certain qu'au moment de la r√©daction, d'autres LLM pourraient supplanter ceux pr√©sent√©s, et c'est une excellente nouvelle pour l'innovation. Cependant, ces mod√®les ont √©t√© choisis pour √©tablir la base de r√©f√©rence (baseline) de cette premi√®re analyse. Cette approche √©tait indispensable pour des raisons de faisabilit√© et d'optimisation des ressources, privil√©giant l'√©tude approfondie de cet √©chantillon cl√© plut√¥t qu'une multiplication exhaustive des tests.

#### Le fl√©au de l'√©valuation des LLM

- ‚ùì Question cruciale : 
  - Comment [**√©valuer**]{.orange} un LLM ?
  - Classification ‚û°Ô∏è plus facile, vraiment ?
  - Complexit√© de la nomenclature

- Utilisation des $27k$ annotations comme ground truth ü•á
  
- [**3 m√©triques**]{.orange} de performances :
  - Pr√©cision [**totale**]{.blue2}
  - Pr√©cision parmi les [**"codables"**]{.blue2}
  - Pr√©cision [**"LLM"**]{.blue2} (erreurs imputables √† la g√©n√©ration seulement)


#### Performance des LLM {.scollable}




### √âtape 7 : G√©n√©rer le jeu multivoques

#### Reconstruction du jeu multivoques: plusieurs strat√©gies

- üí°[**Principe**]{.orange} : consid√©rer les LLMs comme des [**annotateurs classiques**]{.blue2} ‚û°Ô∏è X-annotation
- ‚ùìPeut-on am√©liorer performances en [**mixant les r√©sultats**]{.blue2} de chaque mod√®les ?
- Construction de [**3 annotations**]{.orange} suppl√©mentaires 
  1. Choix en [**cascade**]{.blue2} (un mod√®le en priorit√©)
  2. Choix par [**vote √† la majorit√©**]{.blue2} 
  3. Choix par [**vote pond√©r√©**]{.blue2}

#### "Plusieurs cerveaux valent mieux qu'un seul": combiner les annotations

```{python}
#| echo: false
#| fig-cap: Performance des LLM et leurs combinaisons avec dans l'ordre Qwen, Ministral, Mistral et Llama 3.1
import pandas as pd
import numpy as np
from plotnine import *

# Create the data
data = {
    'model': ['Qwen'] * 5 + ['Ministral'] * 5 + ['Mistral'] * 5 + ['Llama3.1'] * 5 + ['Cascade'] * 5 + ['Vote'] * 5,
    'level': ['Section', 'Division', 'Groupe', 'Classe', 'Sous-classe'] * 6,
    'accuracy': [.8431, .8213, .7335, .6835, .6657,
                 .9016, .8726, .7729, .7044, .6784,
                 .9110, .8869, .7928, .7375, .7173,
                 .8771, .8508, .7627, .7041, .6836,
                 .9233, .8978, .8063, .7544, .7351,
                 .9215, .8963, .8044, .7529, .7330]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Define the model order (you can adjust this if you want a different order)
model_order = ['Qwen', 'Ministral', 'Mistral', 'Llama3.1', "Cascade", "Vote",]
df['model'] = pd.Categorical(df['model'], categories=model_order, ordered=True)

# Define the correct order of levels
level_order = ['Sous-classe', 'Classe', 'Groupe', 'Division','Section',]

# Convert level to categorical with the specified order
df['level'] = pd.Categorical(df['level'], categories=level_order, ordered=True)

# Format accuracy values for labels
df['accuracy_label'] = df['accuracy'].round(2).astype(str)

# Create the lollipop chart
(ggplot(df, aes(x='model', y='accuracy'))
 + geom_segment(aes(x='model', xend='model', y=0, yend='accuracy'))
 + geom_point(size=3, color='blue')
 + geom_text(aes(label='accuracy_label'), va='bottom', ha='center', 
             size=8, nudge_y=0.02)
 + facet_wrap('~level', ncol=3)
 + theme_minimal()
 + labs(
     x='',
     y=''
 )
 + theme(
     figure_size=(12, 6),  # Increased width to accommodate labels
     panel_spacing=0.05,
     axis_text=element_text(size=9),
     axis_title=element_text(size=12),

 )
 + scale_y_continuous(limits=[0, 1.05], breaks=np.arange(0, 1.1, 0.1))  # Increased upper limit to fit labels
)
```

Transition: Une premi√®re base d'apprentissage obtenu, il reste √† entra√Æner un mod√®le en NAF 2025 et pouvoir le qualifier pour un passage en production