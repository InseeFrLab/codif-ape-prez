Cette partie pr√©sente une m√©thodologie innovante visant √† reconstruire l‚Äôexhaustivit√© de la base d‚Äôapprentissage en tirant parti des LLM, et plus particuli√®rement des mod√®les comp√©titifs publi√©s en open source. L‚Äôapproche s‚Äôappuie sur l‚Äôexpertise des annotateurs, les notes explicatives de la DNE et l‚Äôinspiration de projets r√©ussis dans d‚Äôautres INS, comme @ClassifAI. La puissance de calcul du Datalab SSP Cloud permet de tester cette approche prometteuse et de l‚Äôadapter aux besoins m√©tier sp√©cifiques de la base Sirene, en int√©grant le passage √† la NAF 2025.

L‚Äôutilisation de donn√©es open data √©vite les contraintes d‚Äôanonymisation, et l‚Äôauto-h√©bergement avec les technologies cloud assure ind√©pendance et reproductibilit√© sur des infrastructures similaires pour tous les utilisateurs du SSP Cloud.

## M√©thodologie 

Objectif : Reconstruire un jeu d‚Äôentra√Ænement le plus exhaustif possible
Exp√©rimentation one-shot ‚û°Ô∏è pas de contrainte de reproductibilit√© ‚Ä¶ en production
Utilisation de LLMs pour g√©n√©rer des annotations en NAF 2025

Pour la suite, les r√©sultats s'appuient sur l'utilisation du CAG.
L‚Äôid√©e est de consid√©rer les LLM comme des **annotateurs humains**, en imitant leur d√©marche. Particuli√®rement, le mode op√©ratoire pr√©sent√© dans la partie pr√©c√©dente. 

Les LLM sur la base des notes explicatives attribuent le code NAF 2025 le plus appropri√© parmi les choix propos√©s. Lorsqu'il ne sait pas, il consid√®re la t√¢che comme inclassable. La base recod√© r√©sultante correspondra √† celle que le LLM aura r√©ussi √† recoder. Il y aura donc des pertes mais une partie pr√©pond√©rante de la base multivoque sera recod√©, ce sera la nouvelle et une initialisation de la base d'apprentissage en NAF 2025 pour un classifieur.

rappeler l'objectif dans cette partie, ne pas construire de classifieur bas√© sur LLM mais construire annotation

::: {.center}
![Approche globale pour construire le mod√®le en NAF 2025](img/methodo-nace-rev.png){#fig-methodo-nace-rev}

*Note de lecture : TODO*
:::

## Contexte technique et ressources disponibles

### Le datalab SSP Cloud comme plateforme d'exp√©rimentation

Toutes les technologies cit√©es sont auto-h√©berg√©es et open-source.
Bien que les donn√©es ne soient pas sensibles, pas d'utilisation d'offre propri√©taire

LLM environnement production ? Non Type d'op√©ration : exp√©rimentale et ponctuelle
Avantage de l'open source 

#### souverainet√© et ind√©pendance: auto-h√©bergement

#### Des ressources mat√©rielles pr√©cieuses

GPU H100, CPU, S3

#### Des services √† disposition pour les traitements IA

R√©f√©rences : *mettre les technologies cloud au service de la production publique*. 
@cloudprod 

##### Base de donn√©es vectorielles

Milvius, ChromaDB, Qdrant

##### Serveur d'inf√©rence optimis√©

vllm pour optimiser le calcul sur GPU
d√©fi de l'inf√©rence par batch ==> besoin de vitesse tr√®s rapide
sp√©cificit√© de l'inf√©rence par batch. D√©fis r√©cents
domaine tr√®s r√©cent
flash-attention

### Gestionnaire de prompts

Plateforme Langfuse

### Gestionnaire de mod√®les MLflow

Plateforme Langfuse

## D√©tail des √©tapes et concepts cl√©s appliqu√©s

### √âtape 1: Pr√©parer les donn√©es √† disposition

Principalement des extractions de donn√©es pr√©par√©es par le m√©tier
Base d'apprentissage issue de Sirene 4
Mat√©riaux des experts de la nomenclature (DNE)
Table de correspondance th√©orique
Annotation manuelle comme ground truth

chargement en bdd vectorielle

### √âtape 2 : Chargement de LLM

Qu'est ce qu'un LLM ?
Hugging Face

### √âtape 3: Param√©trer le LLM

#### Prompting

Qu'est ce qu'un prompt ? description textuelle de la t√¢che qu'une IA doit effectuer

##### Donner une feuille de route au LLM: le prompt syst√®me, un prompt structurant

un prompt particulier
Prompt syst√®me:

_manuel d‚Äôinstruction lu par les IA g√©n√©ratives avant de vous r√©pondre_

```
Tu es un expert de la Nomenclature statistique des Activit√©s √©conomiques dans la Communaut√© Europ√©enne (NACE). Tu es charg√© de r√©aliser le changement de nomenclature. Ta mission consiste √† attribuer un code NACE 2025 √† une entreprise, en t'appuyant sur le descriptif de son activit√© et √† partir d'une liste de codes propos√©s (identifi√©e √† partir de son code NACE 2008 existant). Voici les instructions √† suivre :
1. Analyse la description de l'activit√© principale de l'entreprise et le code NACE 2008 fourni par l'utilisateur.
2. √Ä partir de la liste des codes NACE 2025 disponible, identifie la cat√©gorie la plus appropri√©e qui correspond √† l'activit√© principale de l'entreprise.
3. Retourne le code NACE 2025 au format JSON comme sp√©cifi√© par l'utilisateur. Si la description de l'activit√© de l'entreprise n'est pas suffisamment pr√©cise pour identifier un code NACE 2025 ad√©quat, retourne `null` dans le JSON.
4. √âvalue la coh√©rence entre le code NACE 2008 fourni et la description de l'activit√© de l'entreprise. Si le code NACE 2008 ne semble pas correspondre √† cette description, retourne `False` dans le champ `nace08_valid` du JSON. Note que si tu arrives √† classer la description de l'activit√© de l'entreprise dans un code NACE 2025, le champ `nace08_valid` devrait `True`, sinon il y a incoh√©rence.
5. R√©ponds seulement avec le JSON compl√©t√© aucune autres information ne doit √™tre retourn√©.
```
##### Le prompt utilisateur, une requ√™te contextuelle au LLM
 Prompt utilisateur ou Requ√™te:
 - Un prompt [**sp√©cifique**]{.orange} pour chaque observations comprenant : 
  - le [**libell√©**]{.blue2} de l'activit√© principale de l'entreprise
  - l'ancien [**code NAF 2008**]{.blue2} connu
  - La [**liste des codes possibles**]{.blue2} issues du mapping avec leurs [**notes explicatives**]{.blue2}

- Une [**instruction sur le format**]{.orange} de r√©ponse attendu

### √âtape 4: Apporter le contexte au LLM

G√©n√©ration augment√©e, insufissance de la base de connaissance

#### RAG

@rag_paper

#### CAG 

@cag_paper

Quelle stat√©gie utiliser ?

R√©ponse: contexte m√©tier 

Lequel utiliser ? Don't do RAG ? Oui et non, √† nuancer mais du coup

avantage/inconv√©nients: retrieval imm√©diat mais propagation de l'erreur en NAF rev 2
Int√©ressant si bonne classification en NAF rev 2, notre cas donc CAG

### √âtape 5: G√©n√©rer les annotations

Contr√¥ler la sortie du LLM: sp√©cification et formatage

Qualifier la r√©ponse g√©n√©r√©e

- Tendance des LLMs a √™tre tr√®s [**volubiles**]{.orange}...
- Construction d'une [**r√©ponse type**]{.orange}, claire et br√®ves dans un [**format sp√©cifique**]{.orange}
- [**Format JSON**]{.orange} est le plus abondamment utilis√© dans l'√©cosyst√®me

```json
{
    "codable": true,
    "nace_2008_valid": true,
    "nace2025": "0147J" 
}
```


- [**Parsing**]{.orange} de la r√©ponse :
  1. [**V√©rification**]{.blue2} du format
  2. Contr√¥le des [**hallucinations**]{.blue2}

### √âtape 6 : √âvaluation des LLM

#### Quels LLM utiliser ?

| Mod√®le | Taille | Vitesse d'inf√©rence | Performance | Caract√©ristique |
| --------- | ----------- | ---------- | ------------- | ------- |
| Qwen 2.5 | 32B | [Lent]{.orange} | [Bonnes performances]{.green2} | Tr√®s restrictif |
| Ministral | 8B | [Extr√™mement rapide]{.green2} | [Tr√®s raisonnable]{.orange} | Pas restrictif |
| Mistal Small | 44B | [Lent]{.orange} | [Bonnes performances]{.green2} | Assez restrictif |
| Llama 3.1 | 70B (quantis√©) | [Extr√™mement lent]{.red2} | [Tr√®s bonnes performances]{.green2} | Assez restrictif |

La s√©lection des Large Language Models (LLM) s'est concentr√©e sur les mod√®les les plus reconnus, consid√©r√©s comme State-of-the-Art (SOTA) √† un moment donn√©, ou **disponibles en open source**. Il est certain qu'au moment de la r√©daction, d'autres LLM pourraient supplanter ceux pr√©sent√©s, et c'est une excellente nouvelle pour l'innovation. Cependant, ces mod√®les ont √©t√© choisis pour √©tablir la base de r√©f√©rence (baseline) de cette premi√®re analyse. Cette approche √©tait indispensable pour des raisons de faisabilit√© et d'optimisation des ressources, privil√©giant l'√©tude approfondie de cet √©chantillon cl√© plut√¥t qu'une multiplication exhaustive des tests.

#### Le fl√©au de l'√©valuation des LLM

- ‚ùì Question cruciale : 
  - Comment [**√©valuer**]{.orange} un LLM ?
  - Classification ‚û°Ô∏è plus facile, vraiment ?
  - Complexit√© de la nomenclature

- Utilisation des $27k$ annotations comme ground truth ü•á
  
- [**3 m√©triques**]{.orange} de performances :
  - Pr√©cision [**totale**]{.blue2}
  - Pr√©cision parmi les [**"codables"**]{.blue2}
  - Pr√©cision [**"LLM"**]{.blue2} (erreurs imputables √† la g√©n√©ration seulement)


#### Performance des LLM {.scollable}
```{python}
#| echo: false
#| fig-cap: Performance des LLM 
import pandas as pd
import numpy as np
from plotnine import *

# Create the data
data = {
    'model': ['Qwen', 'Qwen', 'Qwen',
              'Ministral', 'Ministral', 'Ministral',
              'Mistral', 'Mistral', 'Mistral',
              'Llama3.1', 'Llama3.1', 'Llama3.1'],
    'accuracy_type': ["Pr√©cision totale", 'Pr√©cision "LLM"', 'Pr√©cision parmi les "codables"'] * 4,
    'accuracy': [.6657, .7397, .7300,
                .6784, .7538, .6786,
                .7173, .7971, .7238,
                .6836, .7688, .7209]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Define the model order (you can adjust this if you want a different order)
model_order = ['Qwen', 'Ministral', 'Mistral', 'Llama3.1']
df['model'] = pd.Categorical(df['model'], categories=model_order, ordered=True)

accuracy_order = ["Pr√©cision totale", 'Pr√©cision "LLM"', 'Pr√©cision parmi les "codables"']
df['accuracy_type'] = pd.Categorical(df['accuracy_type'], categories=accuracy_order, ordered=True)

# Format accuracy values for labels
df['accuracy_label'] = df['accuracy'].round(2).astype(str)

# Create the lollipop chart
(ggplot(df, aes(x='model', y='accuracy'))
 + geom_segment(aes(x='model', xend='model', y=0, yend='accuracy'))
 + geom_point(size=3, color='blue')
 + geom_text(aes(label='accuracy_label'), va='bottom', ha='center', 
             size=8, nudge_y=0.02)
 + facet_wrap('~accuracy_type', ncol=3)
 + theme_minimal()
 + labs(
     x='',
     y=''
 )
 + theme(
     figure_size=(12, 4.5),  # Increased width to accommodate labels
     panel_spacing=0.05,
     axis_text=element_text(size=10),
     axis_title=element_text(size=12),

 )
 + scale_y_continuous(limits=[0, 1.05], breaks=np.arange(0, 1.1, 0.1))  # Increased upper limit to fit labels
)
```



### √âtape 7 : G√©n√©rer le jeu multivoques

#### Reconstruction du jeu multivoques: plusieurs strat√©gies

- üí°[**Principe**]{.orange} : consid√©rer les LLMs comme des [**annotateurs classiques**]{.blue2} ‚û°Ô∏è X-annotation
- ‚ùìPeut-on am√©liorer performances en [**mixant les r√©sultats**]{.blue2} de chaque mod√®les ?
- Construction de [**3 annotations**]{.orange} suppl√©mentaires 
  1. Choix en [**cascade**]{.blue2} (un mod√®le en priorit√©)
  2. Choix par [**vote √† la majorit√©**]{.blue2} 
  3. Choix par [**vote pond√©r√©**]{.blue2}

#### "Plusieurs cerveaux valent mieux qu'un seul": combiner les annotations

```{python}
#| echo: false
#| fig-cap: Performance des LLM et leurs combinaisons avec dans l'ordre Qwen, Ministral, Mistral et Llama 3.1
import pandas as pd
import numpy as np
from plotnine import *

# Create the data
data = {
    'model': ['Qwen'] * 5 + ['Ministral'] * 5 + ['Mistral'] * 5 + ['Llama3.1'] * 5 + ['Cascade'] * 5 + ['Vote'] * 5,
    'level': ['Section', 'Division', 'Groupe', 'Classe', 'Sous-classe'] * 6,
    'accuracy': [.8431, .8213, .7335, .6835, .6657,
                 .9016, .8726, .7729, .7044, .6784,
                 .9110, .8869, .7928, .7375, .7173,
                 .8771, .8508, .7627, .7041, .6836,
                 .9233, .8978, .8063, .7544, .7351,
                 .9215, .8963, .8044, .7529, .7330]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Define the model order (you can adjust this if you want a different order)
model_order = ['Qwen', 'Ministral', 'Mistral', 'Llama3.1', "Cascade", "Vote",]
df['model'] = pd.Categorical(df['model'], categories=model_order, ordered=True)

# Define the correct order of levels
level_order = ['Sous-classe', 'Classe', 'Groupe', 'Division','Section',]

# Convert level to categorical with the specified order
df['level'] = pd.Categorical(df['level'], categories=level_order, ordered=True)

# Format accuracy values for labels
df['accuracy_label'] = df['accuracy'].round(2).astype(str)

# Create the lollipop chart
(ggplot(df, aes(x='model', y='accuracy'))
 + geom_segment(aes(x='model', xend='model', y=0, yend='accuracy'))
 + geom_point(size=3, color='blue')
 + geom_text(aes(label='accuracy_label'), va='bottom', ha='center', 
             size=8, nudge_y=0.02)
 + facet_wrap('~level', ncol=3)
 + theme_minimal()
 + labs(
     x='',
     y=''
 )
 + theme(
     figure_size=(12, 6),  # Increased width to accommodate labels
     panel_spacing=0.05,
     axis_text=element_text(size=9),
     axis_title=element_text(size=12),

 )
 + scale_y_continuous(limits=[0, 1.05], breaks=np.arange(0, 1.1, 0.1))  # Increased upper limit to fit labels
)
```

Transition: Une premi√®re base d'apprentissage obtenu, il reste √† entra√Æner un mod√®le en NAF 2025 et pouvoir le qualifier pour un passage en production