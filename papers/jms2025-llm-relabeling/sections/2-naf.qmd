Cette partie abordera les conséquences de la révision de la nomenclature NAF, tant au niveau de sa structure que sur le volume de travail qu'elle représente pour les données d'entraînement. En particulier, les enjeux soulevés par l'exercice d'annotation en NAF 2025 y seront détaillés. Cette expérience permettra d'alimenter la matière première nécessaire à la méthodologie innovante qui sera présentée dans la partie suivante. Enfin, cette section dédiée à la tâche humaine permettra de mettre ultérieurement en perspective l'exercice manuel par rapport à l'utilisation des Large Language Models (LLM) dans la partie suivante.

## Les conséquences du changement de nomenclature

### la table de correspondance théorique

Comme documenté par le rapport du groupe de travail du CNIS, la révision de la NAF pour 2025 implique d'importantes modifications structurelles, notamment des regroupements de codes mais aussi de nombreuses scissions. Pour constater le détail de ces évolutions, il convient de se référer à l'Annexe 10 - Bilan détaillé de l'instruction, classe par classe du rapport @rev_naf_2025.

::: { .center}
| Niveau de la Nomenclature | Terme (Intitulé) | Nombre de Positions | 
| :--- | :--- | :--- | 
| Niveau 1 | Section | 1 (Lettre) | 
| Niveau 2 | Division | 2 (Chiffres) | 
| Niveau 3 | Groupe | 3 (Chiffres) | 
| Niveau 4 | Classe | 4 (Chiffres) | 
| Niveau 5 | Sous-classe | 5 (Chiffres + Lettre) |

: Quelques éléments de vocabulaire sur la structure de la NAF {#tbl-vocab-naf}
:::

Tout au long de cet article, nous emploierons les termes univoques et multivoques pour caractériser ces changements, selon les définitions détaillées ci-dessous

#### Univoques

Les correspondances **univoques** (ou bijections) désignent les cas où un code de l'ancienne nomenclature (NAF rev 2 ou NAF 2008) correspond à un seul code dans la nouvelle nomenclature (NAF 2025). Ces cas ne présentent pas de difficulté majeure pour la conversion automatique des données d'apprentissage.

::: {.center}
![Exemple de correspondance univoque 1:1](img/univoques.png){#fig-bijection width=60%}
:::
*Cette figure illustre une correspondance de type 1:1 entre un code de l'ancienne NAF et un code unique de la NAF 2025.*

::: {.center}
|   Niveau classe (niveau 4) |   Niveau sous-classe (niveau 5) |
|---------------------------:|--------------------------------:|
|                        573 |                             551 |

: Effectif des codes univoques ou bijections. Cette table indique qu'une large majorité des codes, 75%, sont univoques.  {#tbl-dist-univoques}
:::

#### Multivoques

Les cas multivoques représentent les situations où l'expertise humaine est indispensable, car ils introduisent une ambiguïté dans la conversion. Deux cas de figures se présentent mais l'attention se portera sur un cas particulier de correspondance.

::: {.center}
![Exemple de correspondance N:1 ou surjection](img/surjection.png){#fig-surjection width=60%}
:::
*Cette figure illustre une surjection (N:1) où plusieurs anciens codes sont regroupés dans un seul nouveau code*

Les surjections (correspondances N:1) ne posent pas de problème fonctionnel majeur dans le système de codification, car pour chaque ancien code considéré individuellement, il n'y a qu'un choix possible dans la nouvelle nomenclature. Elles peuvent donc être traitées de manière univoque dans le sens de la conversion.

::: {.center}
![Exemple de correspondance multivoque 1:N](img/multivoques.png){#fig-multivoques width=60%}
:::
*Cette figure illustre un éclatement (1:N) où un ancien code correspond à plusieurs codes possibles de la NAF 2025.*

Les **multivoques désigneront**, tout au long de cet article, principalement les correspondances 1:N (éclatements), où N représente le nombre de possibilités dans la nouvelle nomenclature. Ces cas, qui concernent 25 % des codes, sont ceux qui requièrent une annotation manuelle pour déterminer la sous-classe correcte en NAF 2025 associé au libellé dans le jeu d'apprentissage, dont la conversion sera approfondie dans la section qui suit.

::: {#tbl-dist-multivoques layout-ncol=2}
|   1-to-N |   Occurrences |
|---------:|--------------:|
|        2 |           101 |
|        3 |            28 |
|        4 |            15 |
|        5 |             3 |
|        6 |             5 |
|        7 |             1 |
|        8 |             1 |
|       17 |             1 |
|       25 |             1 |
|       29 |             1 |
|       30 |             2 |

:  Niveau classe: 4 positions {#tbl-dist-multivoques-niv4}

|   1-to-N |   Occurrences |
|---------:|--------------:|
|        2 |           109 |
|        3 |            30 |
|        4 |            24 |
|        5 |             6 |
|        6 |             4 |
|        8 |             1 |
|        9 |             2 |
|       21 |             1 |
|       27 |             1 |
|       36 |             1 |
|       38 |             2 |

: Niveau sous-classe: 5 positions {#tbl-dist-multivoques-niv5}

Distribution des codes multivoques pour la NAF
:::

::: {.center}

|   Part de multivoques  |   Part d'univoques   |
|---------------------------:|--------------------------------:|
|                        25 % |                             75 % |


: Représentation des multivoques dans la table de correspondance {#tbl-dist-multivoques-corresp}
:::

### Le jeu d'apprentissage

Le jeu d'apprentissage est fixé sur les données de l'année 2024 pour servir de base de travail à l'ensemble de l'article.

Afin de rationner les ressources d'annotation, une première étape consiste à séparer les codes univoques (1:1), qui peuvent être facilement convertis en NAF 2025 via la table de correspondance théorique. La problématique se restreint alors à ne traiter que les codes multivoques. La question est de savoir quel volume de données ces derniers représentent.

#### Un volume de multivoques toutefois plus conséquent 

Bien que les codes multivoques ne représentent que 25% des codes dans la nomenclature elle-même (comme établi dans la section précédente), leur fréquence d'apparition dans les données d'apprentissage est importante. Ils constituent en effet 52% du volume total des données d'entraînement à convertir en NAF 2025. Cette proportion révèle l'ampleur du travail d'annotation nécessaire pour obtenir une vérité terrain fiable.

::: {.center}

|   Part de multivoques  |   Part d'univoques   |
|---------------------------:|--------------------------------:|
|                        52 % |                             48 % |

: Représentation des multivoques dans le jeu d'apprentissage {#tbl-dist-multivoques-data}
:::
*Malgré un impact sur seulement 25% des codes de la NAF, ces cas multivoques représentent l'essentiel du volume des données d'entraînement à convertir en NAF 2025.*

#### Une approche métier insuffisante pour réduire le nombre

Une stratégie métier a été mise en place pour réduire ce volume. Elle consiste en la requalification de certains codes multivoques en correspondances biunivoques sur la base du principe de réalité métier et des informations contextuelles disponibles dans la déclaration.

Cette table de passage métier permet d'identifier et de résoudre les "faux multivoques"[^remarque-tbl-passage]. Prenons l'exemple du code 6820A ("Location de logements") : bien qu'il puisse théoriquement correspondre à plusieurs nouveaux codes NAF, l'analyse manuelle montre qu'il est recodé en 6820G dans la grande majorité des cas multivoques (16% des cas ambigus). Le postulat métier est alors retenu que cette situation est univoque, car ces locations sont majoritairement de longue durée. De même, les activités de production d'électricité des personnes physiques (3511Z) sont approximées comme de l'électricité de type renouvelable (3512Y), en supposant qu'il s'agit principalement de la production par panneaux solaires.

::: {.center}

|      Type d'activité principale concerné       | Code initial | Code final | Proportion | Parti pris métier |
|----------------------------------------------------:|-------------:|-----------:|------------:|------------------------------------:|
|                  Location de logements |        6820A |      6820G |            16% | Très souvent de longue durée |
|          Location de biens immobiliers |        6820B |      6820H |           10 % |                              |
|               Production d'électricité |        3511Z |      3512Y |            3 % | Type renouvelable, si personne physique |
|            Restauration traditionnelle |        5610A |      5611G |            2 % | Service principalement à table |
|                 Réparation de voitures |        4520A |      9531G |          1,5 % | Quasi-absence d'intermédiation |
|       Commerce d'alimentation générale |        4711B |      4711G |          0,5 % | Rarement intermédiation ou vente en ligne|
|  Construction de maisons individuelles |        4120A |      4100G |          0,5 % | Exclure la restauration de monuments |

: Requalification de certaines correspondances en biunivoque dans le jeu  : {#tbl-dist-faux-multivoques-niv5}
:::

Grâce à cette approche métier, 32,5 %, soit près d'un tiers, des codes multivoques initialement identifiés ont pu être corrigés ou reclassifiés dans la base d'apprentissage. Cependant, cette méthode n'est applicable qu'à moins de 50 % du volume total des cas multivoques. Étant donné que la base d'apprentissage complète (univoques et multivoques) atteint deux millions d'observations, et qu'un nombre important de cas ambigus subsiste même après cette première reclassification, il resterait environ 500 000 multivoques à convertir. Ce volume résiduel est trop important pour être traité manuellement, car il nécessiterait des moyens humains importants que l'équipe n'a pas à disposition. Cela démontre l'inefficacité de cette méthode pour une conversion à grande échelle et souligne la nécessité de mettre en place un outil de codage automatique pour traiter ces cas de manière exhaustive.

[^remarque-tbl-passage]: Certains codes dont le 6820B et le 4520A ont, au final, été corrigé lors de la mise à jour de la table de correspondance officielle, confirmant l'hypothèse de départ de reclassification.

## La campagne d'annotation: à l'origine de nos matériaux

En l'absence totale de données étiquetées en NAF 2025, une campagne d'annotation a été menée pour constituer la première vérité terrain exploitable. Ce travail était indispensable pour plusieurs raisons : il permettait non seulement de faire vivre la nouvelle nomenclature en conditions réelles et de préparer les futurs gestionnaires, mais aussi d'obtenir un retour concret sur sa structure. L'objectif était également d'affiner la table de correspondance théorique, de soulever les cas de classement récurrents qui demandent une décision métier pour arbitrage, et d'assurer une mobilisation progressive des équipes.

### Déroulé 

La campagne d'annotation a été gérée dynamiquement grâce à une plateforme de labellisation[^label-studio], mise à disposition par l'équipe métier. L'approche méthodologique était simple : les experts réalisaient une annotation simple pour chaque libellé. La plateforme garantissait nativement l'accès concurrent aux données, s'assurant qu'un libellé n'était vu et annoté qu'une seule fois par un expert donné. Les participants pouvaient aussi, pour chaque tâche, attribuer un commentaire et une note de 0 à 5 étoiles sur la difficulté ressentie[^rating-note]. Les consignes mettaient l'accent sur la qualité du codage plutôt que sur la quantité, avec la possibilité de déléguer un libellé inclassable dans la nomenclature au sein d'un "pot commun" pour qu'il soit reproposé à un autre expert annotateur. Lorsqu'un libellé ne décrivait pas clairement une activité ou était inclassable (par manque de précision, ambiguïté, etc.), l'annotateur pouvait appliquer un code **inclassable**. Le travail s'est appuyé sur les tables de correspondance et les notes explicatives disponibles. Initialement, une charge de travail d'environ une heure par jour était recommandée pour l'ensemble des participants.

[^rating-note]: la note la plus élevée indiquant la plus grande difficulté
[^label-studio]: il s'agit de la version open source de Label Studio

Au niveau du calendrier, la campagne s'est déroulée sur environ 5 mois, après une phase de formation à la NAF 2025 et un bêta-test. Elle a mobilisé près de 25 experts du réseau APE. La campagne a été rythmé par des réunions mensuelles avec la _Division Nomenclatures économiques_ (DNE) et la _Division Répertoire interadministratif Sirene_ (RIAS), toutes deux rattachées au _Département Répertoires, infrastructures et statistiques structurelles_ (DRISS), assurant un retour d'expérience régulier avec les experts de la nomenclature et des administrateurs de Sirene. Ces échanges étaient précieux pour la montée en compétence des annotateurs, la résolution des cas de classement les plus complexes et la gestion collective des cas à trancher. Ces opportunités ont permis d'anticiper certaines consignes de la NAF 2025 et de profiter de l'expertise de la DNE, favorisant un travail stimulant.

L'organisation quotidienne a varié selon les sites, par demi-journée ou selon convenance des agents. Malgré l'intérêt du travail et les échanges de pratiques, la charge prévue d'une heure par jour est devenue difficile à maintenir vers la fin de la campagne en raison de missions annexes.

### Stratégie d'échantillonnage pour l'annotation

Pour garantir la plus grande diversité possible, les libellés soumis à l'annotation ont été échantillonnés de manière distincte au sens strict, ce qui permet d'accepter les variations incluant du bruit ou des coquilles. L'échantillonnage était également stratifié afin d'assurer le codage des libellés les plus fréquents. Cette approche permet de prioriser les cas courants, tout en évitant de traiter les activités très rares qui seraient, de toute façon, destinées à de l'expertise gestionnaire.

Au final, cette approche a permis de constituer un jeu de données très particulier, ne concernant que les multivoques les plus représentatifs de la distribution, par la stratification, tout en assurant la plus grande variété possible sur un échantillon limité.

### Bilan et apports de la campagne

Au niveau national, la campagne a généré un volume important de données et de retours qualitatifs.
Le bilan est le suivant:

- Total annoté : 30 915 libellés.
- Tâches inclassables : 1 830 libellés, appliqués lorsque l'activité n'était pas claire ou manquait de précisions.
- Tâches déléguées : 3 553 libellés, mis de côté pour être reproposés à un autre expert.

#### Documents de référence et vérité terrain

Le principal apport de cette campagne a été la stabilisation précoce des ressources de référence nécessaires à la nouvelle nomenclature. Grâce aux remontées des experts, la structure de la NAF 2025, la table de correspondance théorique et les notes explicatives ont pu être affinées. Surtout, cet exercice a permis de constituer la première vérité terrain exploitable, un matériau précieux préparant l'ensemble de la méthodologie suivante.

#### Mode opératoire des experts 

La campagne a permis de mettre au point une feuille de route face aux cas ambigus. Pour guider l'annotation, un arbre de décision a été formalisé, décrivant la séquence logique suivie par les experts pour attribuer un code NAF 2025 à un libellé donné. Ce mode opératoire est illustré ci-dessous.

::: {.center}
![Arbre de décision pour coder en NAF 2025](img/arbre-decision-annotation.drawio.png){#fig-decision-tree}

*Cet arbre de décision inspirera fortement la méthodologie décrite dans la partie suivante*
:::

#### Evaluation de la difficulté de la campagne

Afin de collecter des retours sur l'effort cognitif, la plateforme de labellisation a permis aux participants d'attribuer une note de difficulté à chaque tâche selon une échelle simple de cinq modalités. Bien que cet indicateur ne présente pas une rigueur absolue, il constitue la seule mesure de difficulté disponible pour l'évaluation de la campagne.

Les modalités de la note de difficulté sont les suivantes:

- 0 étoile: aucune difficulté ou rien à signaler 
- 1 étoile: plutôt facile
- 2 étoiles: facile 
- 3 étoiles: moyenne
- 4 étoiles: difficile 
- 5 étoiles: très difficile

{{< pagebreak >}}

**Bilan global**

- 0 étoile: 28,85 %  
- 1 étoile: 56,47 % 
- 2 étoiles: 6,17 %
- 3 étoiles: 6,48% 
- 4 étoiles: 1,19 % 
- 5 étoiles: 0,1 %

L'exercice d'annotation est jugé de difficulté globalement modéré par les experts, avec seulement quelques cas jugés très complexes. Une analyse est nécessaire pour déterminer si ces cas difficiles correspondent nécessairement à des libellés issus des multivoques présentant le plus grand nombre de possibilités.

#### Difficulté de l'exercice et nombre de multivoques: un lien pas si immédiat

Les figures ci-dessous illustrent la difficulté ressentie par les experts en fonction du nombre de correspondances multivoques possibles (N) en NAF 2025 pour un code NAF rev 2 donné.

::: {#fig-deux-images layout-nrow=2}
![Moyenne des avis bruts au niveau classe](img/difficulte_brute_niv4.png){#fig-diff-brute-niv4 width=90%}

![Moyenne des avis bruts au niveau sous-classe](img/difficulte_brute_niv5.png){#fig-diff-brute-niv5 width=90%}

Difficulté ressentie en fonction des multivoques de l'exercice d'annotation
:::

L'analyse de ces résultats révèle que la difficulté ressentie n'est pas une fonction croissante simple du nombre de multivoques. La difficulté perçue par les répondants reste globalement stable pour les cas présentant entre 10 et 20 options. Une montée progressive de la charge cognitive n'est observée qu'à partir d'une vingtaine de correspondances possibles, et s'accentue nettement autour d'un seuil de 30 options au niveau classe et 35 options au niveau sous-classe.

Les activités jugées particulièrement difficiles ne sont pas uniquement associées au plus grand nombre de multivoques. Par exemple, le code 4399D (Autres travaux spécialisés de construction), avec seulement N=8 correspondances possibles, est également concerné par les pics de difficulté, tout autant que le 4799A (Vente à domicile, N=38) et le 4791B (Vente à distance sur catalogue spécialisée, N=36).

En particulier, **la difficulté de classement de la vente à domicile et à distance a une explication connue des experts métier**. Elle provient du fait que les libellés soumis ne précisent souvent pas le type de produits vendus, une information non requise par la NAF rev 2 mais essentielle pour le classement en NAF 2025. Les libellés sont donc parfaitement classables sous l'ancienne nomenclature, mais de fait inclassables en NAF 2025. Par conséquent, un nombre plus restreint de multivoques pour cette sous-classe n'aurait pas rendu la tâche plus facile.

Cette observation met en évidence que d'autres facteurs métier influencent la difficulté, au-delà du simple nombre de choix. L'ensemble de ces cas complexes ne représente au final que 2 077 tâches, soit moins de 7% de l'annotation totale.

::: {#fig-deux-images layout-nrow=2}
![Part de jugés difficiles au niveau classe](img/part_difficiles_niv4.png){#fig-part-difficiles-niv4 width=90%}

![Part de jugés difficiles au niveau sous-classe](img/part_difficiles_niv5.png){#fig-part-difficiles-niv5 width=90%}

Proportion de cas jugés difficiles (avis supérieur à 3/5) en fonction des multivoques de l'exercice d'annotation
::::

L'analyse de la proportion de cas jugés difficiles confirme cette observation : un nombre élevé de correspondances multivoques n'entraîne pas nécessairement une augmentation proportionnelle de la part de tâches difficiles. Le même phénomène est observé lorsque l'on analyse les cas déclarés inclassables. Ces résultats sont particulièrement intéressants à mettre en perspective avec la _Table 3 – Distribution des codes multivoques pour la NAF_, qui présente la rareté des cas avec un grand nombre d'options.

#### Concordance des annotateurs

Pour évaluer la difficulté de l’annotation et apprécier la solidité de la vérité terrain issue de la campagne, un exercice de contrôle a été réalisé sur un échantillon de **256 libellés**. Pour des raisons de moyens, cet exercice a été mené par **trois volontaires**, notés **A, B et C**, sur leur temps disponible. Ce contrôle qualité a reposé sur une triple annotation, réalisée en aveugle par trois volontaires distincts (désignés ici A, B et C) sur ce même échantillon. Cet exercice, contrairement à la campagne principale, n'autorisait pas les experts à passer la tâche en cas de doute, limitant le biais de sélection. Il ne s’agissait pas d’un audit exhaustif, mais d’un échantillon exploratoire destiné à éclairer la qualité de l’annotation humaine. L’objectif n’était pas d’obtenir un résultat parfait, mais de disposer d’indicateurs objectifs permettant de situer la variabilité naturelle du jugement humain sur ce type de tâche.

##### Deux niveaux de concordance

* **Concordance _absolue_** : les trois annotateurs choisissent exactement le même code.
* **Concordance _partielle_** : au moins deux annotateurs sur trois convergent vers le même code.

Sur cet échantillon, la **concordance absolue** entre les trois annotateurs s’élève à environ 62 %. Ce résultat, relativement modeste, est cohérent avec la nature interprétative du classement NAF, où des nuances existent entre certains codes proches. Il est attendu que la concordance absolue soit sensiblement plus faible que la concordance partielle sur ce type de tâche. 

Afin d’illustrer la variabilité entre annotateurs, le graphique ci-dessous montre la concordance par paire :

```{python}
#| label: fig-concordance-annotateurs-abc
#| echo: false
#| fig-cap: "Graphe de concordance entre les annotateurs A, B et C"
#| fig-align: center
#| fig-width: 3
#| fig-height: 3

import networkx as nx
import matplotlib.pyplot as plt

# 1. Créer le graphe (non dirigé pour les flèches doubles)
G = nx.Graph()

# 2. Définir les nœuds et les positions pour une forme triangulaire
nodes = ['A', 'B', 'C']
# Coordonnées pour un triangle (ajustez si besoin)
pos = {
    'A': (0, 1),
    'B': (1, 0),
    'C': (-1, 0)
}
G.add_nodes_from(nodes)

# 3. Définir les poids/étiquettes
# Note: Nous utilisons l'attribut 'weight' mais affichons le 'label'
weights = {
    ('A', 'C'): '73.05 %',
    ('A', 'B'): '76.17 %',
    ('C', 'B'): '61.72 %'
}

# Ajouter les arêtes avec les poids
for (u, v), label in weights.items():
    G.add_edge(u, v, label=label)

# 4. Préparer le tracé Matplotlib
fig, ax = plt.subplots(figsize=(4, 3))

# Dessiner les nœuds et les arêtes
nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='white', edgecolors='black', linewidths=1.5)
nx.draw_networkx_labels(G, pos, font_size=16, font_weight='bold')

# Dessiner les arêtes avec des flèches bidirectionnelles (approximation)
# NetworkX dessine des lignes, nous devons utiliser des annotations pour simuler les flèches doubles
# et les étiquettes de manière précise.

# 5. Dessiner les flèches bidirectionnelles et étiqueter manuellement
for (u, v), label in weights.items():
    x1, y1 = pos[u]
    x2, y2 = pos[v]
    
    # Calculer le point médian pour le placement du label
    mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2
    
    # Légèrement décaler les flèches pour les rendre visibles
    offset = 0.05 
    
    # Calculer l'angle de la ligne
    angle = plt.np.arctan2(y2 - y1, x2 - x1)
    
    # Ajouter le label au centre (légèrement décalé)
    ax.text(mid_x - 0.05 * plt.np.sin(angle), 
            mid_y + 0.05 * plt.np.cos(angle), 
            label, 
            fontsize=12, color='black', ha='center', va='center')

    # Dessiner les flèches : u -> v
    # Utiliser plt.arrow ou ax.annotate pour des flèches doubles est complexe.
    # On revient à la fonction draw standard mais on peut la personnaliser
    
    nx.draw_networkx_edges(
        G, pos, edgelist=[(u, v)], 
        arrowstyle='<->', 
        arrowsize=20, 
        arrows=True,
        edge_color='#2D7FB2', 
        width=4, 
        ax=ax
    )

ax.set_title("Concordance inter-annotateur", loc='center', fontsize=9, fontweight='bold')
ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-0.2, 1.2)
ax.axis('off') # Masquer les axes
plt.show()
```

Malgré la faiblesse de la concordance pure, soit près de 62 %, l'analyse révèle qu'un accord majoritaire est atteint dans 96 % des cas, ce qui signifie qu'au moins deux annotateurs sur trois ont convergé vers le même code. Cela démontre que l'annotation est robuste dans sa grande majorité, même si les trois experts ne sont pas systématiquement parvenus à une concordance absolue.

Il semble que le taux de concordance pure ait été tiré vers le bas par la divergence prononcée entre deux annotateurs spécifiques, B et C, dont l'accord par paire n'atteint que 62 %. Cette faiblesse de la concordance partielle entre B et C limite l'accord simultané de l'ensemble du triplet.

L’analyse du schéma de concordance met en évidence une hétérogénéité dans les niveaux d’accord entre annotateurs. Deux annotateurs présentent un niveau de concordance sensiblement plus faible, ce qui laisse penser à une divergence d’interprétation ou de sensibilité aux cas plus ambigus. Ce point n’invalide pas l’exercice, mais rappelle que même entre experts motivés et formés, une part de subjectivité subsiste dans le codage NAF, en particulier au niveau 5.[^difficultes-annotation]

[^difficultes-annotation]: Outre des erreurs manifestes, les divergences de codage peuvent être attribuées à plusieurs facteurs, notamment des erreurs de saisie, des différences d'interprétation, l'ambiguïté des cas de classement ou une asymétrie d'information causé par une différence d'expérience.

Cette observation est importante, car elle montre qu’un désalignement marqué entre deux personnes peut réduire la concordance absolue à trois, tout en laissant émerger un consensus majoritaire fiable. Autrement dit, l’absence d’unanimité ne signifie pas absence de cohérence : dans la majorité des cas, deux annotateurs convergent vers la même réponse, ce qui permet à un consensus d’émerger malgré des profils d’annotation différents.

La même analyse menée au **niveau 4** et au **niveau 5** donne les résultats suivants :

::: {.center}
| Pair  |     Niveau 4 |     Niveau 5 |
| :---- | ---------: | ---------: |
| A – B | 76.56% | 73.05% |
| A – C | 80.86% | 76.17% |
| B – C | 79.69% | 61.72% |
:::

Malgré une concordance absolue limitée, la **concordance partielle est élevée**, ce qui signifie que le consensus majoritaire est très souvent atteint :

**Concordance partielle (≥ 2 sur 3)**

- Niveau 4 : **97.27 %**
- Niveau 5 : **96.09 %**

Ce résultat est important : il montre que l’annotation humaine reste **largement convergente**, même si les trois annotateurs ne choisissent pas toujours strictement le même code.

#### Mise en perspective avec la vérité terrain de la campagne

Sur les **256 cas annotés**, **97** correspondaient à des libellés issus de la campagne principale. Cela a permis d’estimer dans quelle mesure un vote majoritaire entre trois annotateurs serait aligné avec le **code officiel retenu lors de la campagne** :

::: {.center}

| Alignement avec le ground truth            | Taux    |
| ------------------------------------------ | ----------- |
|  Au niveau sous-classe (niv 5) | **79.38 %** |
|  au niveau classe (niv 4) | **85.57 %** |
:::

La proximité de ces taux avec les niveaux de concordance observés montre que :

* la vérité terrain issue de l’annotation simple est **cohérente avec un vote majoritaire**,
* les divergences sont souvent **à un niveau de granularité fine** (concordance niveau 4 > niveau 5),
* un désaccord ne signifie pas nécessairement une erreur, mais peut refléter la subtilité du choix final.

**Le rôle du ground truth de l'annotation**

La vérité terrain (ou ground truth) représente l'ensemble des données de référence en NAF 2025. Dans le cadre de cette campagne, elle repose sur le jugement moyen des experts et doit être considérée comme suffisamment acceptée par le métier pour être qualifiée de donnée de référence. Elle est fondée sur la confiance accordée au travail collectif et au consensus majoritaire des annotateurs, en tant que première base de travail.

Quelle lecture pour la suite ?

Ces résultats suggèrent que la vérité terrain construite lors de la campagne est **suffisamment solide** pour servir de référence dans l’évaluation des modèles de la prochaine partie, tout en gardant à l’esprit qu’elle n’est pas exempte d’imperfections comme toute annotation humaine.
Chercher une concordance de 100 % entre un modèle et cette vérité terrain n’est ni réaliste ni souhaitable. L’enjeu est plutôt d’identifier les modèles capables de s’aligner suffisamment bien avec cette référence pour garantir un niveau satisfaisant de qualité de codage.

La section suivante s’inscrit dans cette logique : évaluer le degré d’alignement des LLM comme annotateurs avec cette vérité terrain afin de sélectionner ceux qui offrent le meilleur compromis entre performance, robustesse et cohérence métier.
