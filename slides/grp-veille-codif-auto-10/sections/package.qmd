
## Objectifs

- Distribution d'une architecture de [***deep learning***]{.orange} standardis√©e pour les besoins de [**classification de texte**]{.orange} avec variables cat√©gorielles
- Publication en [***open-source***]{.orange} pour favoriser la collaboration
- A destination d'autres √©quipes de l'Insee, SSM et INS europ√©ens üëê
- Progressivement pouvoir mettre en production du PyTorch

## Le mod√®le PyTorch

- Le mod√®le en tant que tel en PyTorch natif

```{python}
#| label: pytorch-model
#| echo: true
#| fig-cap: "Initializing the torchFastText PyTorch model"

from torchFastText.model import FastTextModel

model = FastTextModel(embedding_dim=80,
                      num_classes=732,
                      num_rows = 20000,
                      )
print(model)

```

## Gestion des variables cat√©gorielles

- V√©ritable ajout par rapport √† la librairie originale : chaque variable cat√©gorielle a une matrice d'embedding associ√©e
- Le [*read-me*](https://github.com/InseeFrLab/torch-fastText) pr√©cise la fa√ßon dont le mod√®le les g√®re

```{python}
#| label: pytorch-model-cat-var
#| echo: true
#| fig-cap: "Initializing the torchFastText PyTorch model with categorical variables handling"

from torchFastText.model import FastTextModel

model = FastTextModel(embedding_dim=80,
                      num_classes=732,
                      num_rows = 20000,
                      categorical_vocabulary_sizes=[10, 20],
                      categorical_embedding_dims=5
                      )
print(model)

```

## Le module Lightning

- La librairie [`Lightning`](https://lightning.ai/docs/pytorch/stable/) est une surcouche de PyTorch qui permet de g√©rer l'entra√Ænement
- Le package fournit √©galement le "module" Lightning qui peut √™tre donn√© en entr√©e au `Trainer`

##

```{python}
#| label: lightning-module
#| echo: true
#| fig-cap: "Initializing the torchFastText Lightning module"

from torchFastText.model import FastTextModel, FastTextModule
import torch

model = FastTextModel(embedding_dim=80,
                      num_classes=732,
                      num_rows = 20000,
                      )

module = FastTextModule(
    model=model,
    loss= torch.nn.CrossEntropyLoss(),
    optimizer=torch.optim.Adam,
    optimizer_params={"lr": 0.001},
    scheduler = None,
    scheduler_params=None
)
print(module)
```

## Le tokenizer {.scrollable}

- L'objet `NGramTokenizer` reprend la m√©thode des ***ngrams*** du [papier original](https://arxiv.org/abs/1607.01759) pour transformer une phrase en une liste de tokens

```{python}
#| label: tokenizer
#| echo: true
#| fig-cap: "Initializing the NGramTokenizer"

from torchFastText.datasets import NGramTokenizer

training_text = ['boulanger', 'coiffeur', 'boucherie', 'boucherie charcuterie']

tokenizer = NGramTokenizer(
    min_n=3, 
    max_n=6, 
    num_tokens= 100,
    len_word_ngrams=2, 
    min_count=1, 
    training_text=training_text
    )

print(tokenizer.tokenize(["boulangerie"])[0])
```


## La classe wrapper {.scrollable}

- A destination d'utilisateurs d√©butants en deep learning
- Orchestre l'ensemble des briques pour lancer un entra√Ænement rapidement

```{python}

#| label: wrapper
#| echo: true
#| eval: false
#| fig-cap: "Launching quickly a training with torchFastText"
#| execute: 
#|  enabled: false
from torchFastText import torchFastText

# Initialize the model
model = torchFastText(
    num_tokens=1000000,
    embedding_dim=100,
    min_count=5,
    min_n=3,
    max_n=6,
    len_word_ngrams=True,
    sparse=True
)

# Train the model
model.train(
    X_train=train_data,
    y_train=train_labels,
    X_val=val_data,
    y_val=val_labels,
    num_epochs=10,
    batch_size=64
)
# Make predictions
predictions = model.predict(test_data)
```

